{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1 : Importing Libraries**"
      ],
      "metadata": {
        "id": "33DTVoQFVJzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from typing import Dict, List, Union, Optional\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import json\n",
        "import yaml\n",
        "import csv\n",
        "import os\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "X8JdSnxbTzH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 : EDA Module**"
      ],
      "metadata": {
        "id": "X9RSbVp1Vqwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class EDAModule:\n",
        "    def __init__(self, df: pd.DataFrame, output_dir='eda_reports'):\n",
        "        self.df = df\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.output_dir = output_dir\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        self.categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "        self.insights = {}\n",
        "\n",
        "    def generate_full_report(self) -> str:\n",
        "        \"\"\"Generate comprehensive EDA report with advanced insights.\"\"\"\n",
        "        self._check_scalability()\n",
        "        self._advanced_statistical_analysis()\n",
        "        self._analyze_distributions()\n",
        "        self._detect_outliers()\n",
        "        self._recommend_feature_engineering()\n",
        "        self._analyze_correlations()\n",
        "        return self._format_insights()\n",
        "\n",
        "    def _check_scalability(self):\n",
        "        \"\"\"Perform scalability and performance checks.\"\"\"\n",
        "        self.insights['scalability'] = {\n",
        "            'total_memory_usage': float(self.df.memory_usage(deep=True).sum() / 1024**2),\n",
        "            'row_count': int(len(self.df)),\n",
        "            'column_count': int(len(self.df.columns)),\n",
        "            'sparsity_ratio': float(1 - (self.df.count().sum() / (self.df.shape[0] * self.df.shape[1])))\n",
        "        }\n",
        "\n",
        "    def _advanced_statistical_analysis(self):\n",
        "        \"\"\"Conduct advanced statistical analysis.\"\"\"\n",
        "        numeric_insights = {}\n",
        "        for col in self.numeric_cols:\n",
        "            numeric_insights[col] = {\n",
        "                'skewness': float(self.df[col].skew()),\n",
        "                'kurtosis': float(self.df[col].kurtosis()),\n",
        "                'coefficient_of_variation': float(self.df[col].std() / self.df[col].mean()) if self.df[col].mean() != 0 else float('inf'),\n",
        "                'quantiles': {\n",
        "                    '25%': float(self.df[col].quantile(0.25)),\n",
        "                    '50%': float(self.df[col].quantile(0.5)),\n",
        "                    '75%': float(self.df[col].quantile(0.75))\n",
        "                }\n",
        "            }\n",
        "\n",
        "        categorical_insights = {}\n",
        "        for col in self.categorical_cols:\n",
        "            categorical_insights[col] = {\n",
        "                'unique_values_count': int(self.df[col].nunique()),\n",
        "                'top_5_values': dict(self.df[col].value_counts().head()),\n",
        "                'entropy': float(self._calculate_entropy(self.df[col]))\n",
        "            }\n",
        "\n",
        "        self.insights['statistical_analysis'] = {\n",
        "            'numeric_columns': numeric_insights,\n",
        "            'categorical_columns': categorical_insights\n",
        "        }\n",
        "\n",
        "    def _analyze_distributions(self):\n",
        "        \"\"\"Analyze distribution characteristics.\"\"\"\n",
        "        distribution_insights = {}\n",
        "        for col in self.numeric_cols:\n",
        "            _, p_value = stats.normaltest(self.df[col].dropna())\n",
        "            distribution_insights[col] = {\n",
        "                'is_normal_distribution': bool(p_value > 0.05),\n",
        "                'shapiro_wilk_test_p_value': float(p_value)\n",
        "            }\n",
        "        self.insights['distribution_analysis'] = distribution_insights\n",
        "\n",
        "    def _detect_outliers(self):\n",
        "        \"\"\"Detect outliers using IQR method.\"\"\"\n",
        "        outlier_insights = {}\n",
        "        for col in self.numeric_cols:\n",
        "            Q1 = self.df[col].quantile(0.25)\n",
        "            Q3 = self.df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]\n",
        "            outlier_insights[col] = {\n",
        "                'outlier_count': int(len(outliers)),\n",
        "                'outlier_percentage': float(len(outliers) / len(self.df) * 100),\n",
        "                'lower_bound': float(lower_bound),\n",
        "                'upper_bound': float(upper_bound)\n",
        "            }\n",
        "\n",
        "        self.insights['outlier_analysis'] = outlier_insights\n",
        "\n",
        "    def _recommend_feature_engineering(self):\n",
        "        \"\"\"Provide feature engineering recommendations.\"\"\"\n",
        "        recommendations = []\n",
        "        for col in self.numeric_cols:\n",
        "            if self.insights['statistical_analysis']['numeric_columns'][col]['coefficient_of_variation'] > 1:\n",
        "                recommendations.append(f\"Consider binning column {col} due to high variability\")\n",
        "        for col in self.categorical_cols:\n",
        "            if self.insights['statistical_analysis']['categorical_columns'][col]['unique_values_count'] > 10:\n",
        "                recommendations.append(f\"Consider advanced encoding for {col} (e.g., target encoding)\")\n",
        "        self.insights['feature_engineering_recommendations'] = recommendations\n",
        "\n",
        "    def _analyze_correlations(self):\n",
        "        \"\"\"Analyze correlations between numerical columns.\"\"\"\n",
        "        if len(self.numeric_cols) > 1:\n",
        "            correlation_matrix = self.df[self.numeric_cols].corr()\n",
        "            high_correlations = []\n",
        "            for i in range(len(self.numeric_cols)):\n",
        "                for j in range(i + 1, len(self.numeric_cols)):\n",
        "                    col1 = self.numeric_cols[i]\n",
        "                    col2 = self.numeric_cols[j]\n",
        "                    corr = correlation_matrix.loc[col1, col2]\n",
        "                    if abs(corr) > 0.5:\n",
        "                        high_correlations.append({\n",
        "                            'columns': (col1, col2),\n",
        "                            'correlation': float(corr)\n",
        "                        })\n",
        "            self.insights['correlation_analysis'] = high_correlations\n",
        "        else:\n",
        "            self.insights['correlation_analysis'] = \"Insufficient numerical columns for correlation analysis\"\n",
        "\n",
        "    def _calculate_entropy(self, series):\n",
        "        \"\"\"Calculate entropy for categorical variable.\"\"\"\n",
        "        value_counts = series.value_counts(normalize=True)\n",
        "        return -np.sum(value_counts * np.log2(value_counts))\n",
        "\n",
        "    def save_insights(self, format='all'):\n",
        "        \"\"\"Save insights in multiple file formats.\"\"\"\n",
        "        base_filename = f\"{self.output_dir}/eda_insights_{self.timestamp}\"\n",
        "        if format in ['json', 'all']:\n",
        "            with open(f\"{base_filename}.json\", 'w') as f:\n",
        "                json.dump(self.insights, f, indent=4)\n",
        "        if format in ['yaml', 'yml', 'all']:\n",
        "            with open(f\"{base_filename}.yaml\", 'w') as f:\n",
        "                yaml.dump(self.insights, f, default_flow_style=False)\n",
        "        if format in ['csv', 'all']:\n",
        "            self._save_insights_to_csv(base_filename)\n",
        "        if format in ['txt', 'text', 'all']:\n",
        "            with open(f\"{base_filename}.txt\", 'w') as f:\n",
        "                f.write(self._format_insights())\n",
        "        return f\"Insights saved in {base_filename}.*\"\n",
        "\n",
        "    def _save_insights_to_csv(self, base_filename):\n",
        "        \"\"\"Convert nested insights to flat CSV structure.\"\"\"\n",
        "        rows = []\n",
        "        def flatten_dict(d, parent_key='', sep='_'):\n",
        "            items = []\n",
        "            for k, v in d.items():\n",
        "                new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "                if isinstance(v, dict):\n",
        "                    items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "                else:\n",
        "                    items.append((new_key, v))\n",
        "            return dict(items)\n",
        "        for section, content in self.insights.items():\n",
        "            if isinstance(content, dict):\n",
        "                flattened = flatten_dict({section: content})\n",
        "                rows.append(flattened)\n",
        "            elif isinstance(content, list):\n",
        "                for item in content:\n",
        "                    rows.append(flatten_dict({section: item}))\n",
        "        if rows:\n",
        "            keys = set().union(*[d.keys() for d in rows])\n",
        "            with open(f\"{base_filename}.csv\", 'w', newline='') as f:\n",
        "                writer = csv.DictWriter(f, fieldnames=sorted(keys))\n",
        "                writer.writeheader()\n",
        "                writer.writerows(rows)\n",
        "\n",
        "    def _format_insights(self) -> str:\n",
        "        \"\"\"Format insights into a readable report.\"\"\"\n",
        "        report_sections = []\n",
        "        scalability = self.insights['scalability']\n",
        "        report_sections.append(f\"\"\"Scalability Analysis:\n",
        "- Total Memory Usage: {scalability['total_memory_usage']:.2f} MB\n",
        "- Total Rows: {scalability['row_count']}\n",
        "- Total Columns: {scalability['column_count']}\n",
        "- Data Sparsity Ratio: {scalability['sparsity_ratio']:.2%}\"\"\")\n",
        "        report_sections.append(\"\\nDetailed Statistical Analysis:\")\n",
        "        for col_type, columns in self.insights['statistical_analysis'].items():\n",
        "            report_sections.append(f\"\\n{col_type.replace('_', ' ').title()}:\")\n",
        "            for col, stats in columns.items():\n",
        "                report_sections.append(f\"- {col}: {stats}\")\n",
        "        report_sections.append(\"\\nDistribution Analysis:\")\n",
        "        for col, analysis in self.insights['distribution_analysis'].items():\n",
        "            report_sections.append(f\"- {col}: {'Normal' if analysis['is_normal_distribution'] else 'Non-Normal'} Distribution\")\n",
        "        report_sections.append(\"\\nOutlier Analysis:\")\n",
        "        for col, outliers in self.insights['outlier_analysis'].items():\n",
        "            report_sections.append(f\"- {col}: {outliers['outlier_percentage']:.2f}% Outliers\")\n",
        "        report_sections.append(\"\\nFeature Engineering Recommendations:\")\n",
        "        report_sections.append('\\n'.join(self.insights['feature_engineering_recommendations']))\n",
        "        return '\\n'.join(report_sections)\n",
        "\n",
        "    def run_and_save(self, formats=['json', 'yaml', 'txt']):\n",
        "        \"\"\"Run the full report generation and save insights in specified formats.\"\"\"\n",
        "        logger.info(\"Generating full EDA report...\")\n",
        "        self.generate_full_report()\n",
        "        logger.info(\"Saving insights in formats: %s\", formats)\n",
        "        for fmt in formats:\n",
        "            self.save_insights(format=fmt)\n",
        "        logger.info(\"EDA report generation and saving completed.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RS_DdZioAg_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 : Mathematical Query Module**"
      ],
      "metadata": {
        "id": "NMiPXLgKV1BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MathModule:\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "\n",
        "    def execute_math_query(self, query: str) -> Union[float, str]:\n",
        "        \"\"\"Execute mathematical queries with error handling and validation.\"\"\"\n",
        "        try:\n",
        "            if \"difference between\" in query.lower():\n",
        "                col_name = self._extract_column_name(query)\n",
        "                return self._calculate_difference(col_name)\n",
        "            elif \"standard deviation\" in query.lower():\n",
        "                col_name = self._extract_column_name(query)\n",
        "                return self._calculate_std(col_name)\n",
        "            elif \"sum of values\" in query.lower():\n",
        "                return self._calculate_conditional_sum(query)\n",
        "\n",
        "            return \"Query not recognized. Please rephrase.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in math query execution: {str(e)}\")\n",
        "            return f\"Error processing query: {str(e)}\"\n",
        "\n",
        "    def _extract_column_name(self, query: str) -> str:\n",
        "        \"\"\"Extract column name from query string.\"\"\"\n",
        "        for col in self.df.columns:\n",
        "            if col.lower() in query.lower():\n",
        "                return col\n",
        "        raise ValueError(\"Column name not found in query\")\n",
        "\n",
        "    def _calculate_difference(self, col_name: str) -> float:\n",
        "        values = self.df[col_name].dropna().sort_values(ascending=False)\n",
        "        if len(values) < 2:\n",
        "            raise ValueError(\"Insufficient data points\")\n",
        "        return float(values.iloc[0] - values.iloc[1])\n",
        "\n",
        "    def _calculate_std(self, col_name: str) -> float:\n",
        "        return float(self.df[col_name].std())\n",
        "\n",
        "    def _calculate_conditional_sum(self, query: str) -> float:\n",
        "        threshold = float(query.split('greater than')[1].split()[0])\n",
        "        col_name = self._extract_column_name(query)\n",
        "        return float(self.df[self.df[col_name] > threshold][col_name].sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "6n2rqQQUQY1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 : DataQuery Module**"
      ],
      "metadata": {
        "id": "dbUTVa_yV9t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataQueryModule:\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "\n",
        "    def execute_query(self, query: str) -> pd.DataFrame:\n",
        "        \"\"\"Execute data queries with enhanced functionality.\"\"\"\n",
        "        try:\n",
        "            if \"top\" in query.lower():\n",
        "                return self._handle_top_query(query)\n",
        "            elif \"maximum\" in query.lower() or \"minimum\" in query.lower():\n",
        "                return self._handle_extreme_value_query(query)\n",
        "            elif \"group by\" in query.lower():\n",
        "                return self._handle_groupby_query(query)\n",
        "\n",
        "            return pd.DataFrame({\"error\": [\"Query not recognized\"]})\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in data query execution: {str(e)}\")\n",
        "            return pd.DataFrame({\"error\": [str(e)]})\n",
        "\n",
        "    def _handle_top_query(self, query: str) -> pd.DataFrame:\n",
        "        parts = query.lower().split()\n",
        "        n = int(parts[parts.index(\"top\") + 1])\n",
        "        col_name = self._extract_column_name(query)\n",
        "        condition_value = float(query.split(\">\")[1].strip().split()[0])\n",
        "        return self.df[self.df[col_name] > condition_value].head(n)\n",
        "\n",
        "    def _handle_extreme_value_query(self, query: str) -> pd.DataFrame:\n",
        "        col_name = self._extract_column_name(query)\n",
        "        is_max = \"maximum\" in query.lower()\n",
        "        return self.df[self.df[col_name] == (self.df[col_name].max() if is_max else self.df[col_name].min())]\n",
        "\n",
        "    def _handle_groupby_query(self, query: str) -> pd.DataFrame:\n",
        "        group_col = query.split(\"group by\")[1].strip().split()[0]\n",
        "        agg_col = self._extract_column_name(query)\n",
        "        return self.df.groupby(group_col)[agg_col].agg(['mean', 'count', 'sum']).reset_index()\n",
        "\n",
        "    def _extract_column_name(self, query: str) -> str:\n",
        "        \"\"\"Extract column name from query string.\"\"\"\n",
        "        for col in self.df.columns:\n",
        "            if col.lower() in query.lower():\n",
        "                return col\n",
        "        raise ValueError(\"Column name not found in query\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XFQXQR8VQTha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5 : LLM Data Querying**"
      ],
      "metadata": {
        "id": "avMCtVJSWU5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenQAModule:\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        self.model_name = \"facebook/opt-350m\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_direct_column_value(self, query: str) -> str:\n",
        "        \"\"\"Handle direct column value queries\"\"\"\n",
        "        for col in self.df.columns:\n",
        "            if col.lower() in query.lower():\n",
        "                if 'highest' in query.lower() or 'maximum' in query.lower():\n",
        "                    return str(self.df[col].max())\n",
        "                elif 'lowest' in query.lower() or 'minimum' in query.lower():\n",
        "                    return str(self.df[col].min())\n",
        "                elif 'average' in query.lower():\n",
        "                    return str(self.df[col].mean())\n",
        "        return \"Column not found or query not understood\"\n",
        "\n",
        "    def get_answer(self, question: str, context: str) -> str:\n",
        "        try:\n",
        "            prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True\n",
        "            )\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return response.split(\"Answer:\")[1].strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in QA generation: {str(e)}\")\n",
        "            return f\"Error processing query: {str(e)}\"\n",
        "\n",
        "class DataDrivenChatbot:\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        self.eda_module = EDAModule(df)\n",
        "        self.eda_report = self.eda_module.generate_full_report()\n",
        "        self.math_module = MathModule(df)\n",
        "        self.data_query_module = DataQueryModule(df)\n",
        "        self.qa_module = OpenQAModule(df)\n",
        "        self.query_history = []\n",
        "\n",
        "    def process_query(self, query: str) -> Union[str, pd.DataFrame]:\n",
        "        try:\n",
        "            self.query_history.append((datetime.now(), query))\n",
        "\n",
        "            # Direct column value queries\n",
        "            direct_value = self.qa_module.get_direct_column_value(query)\n",
        "            if direct_value != \"Column not found or query not understood\":\n",
        "                return direct_value\n",
        "\n",
        "            if any(word in query.lower() for word in ['calculate', 'sum', 'average', 'deviation']):\n",
        "                return self.math_module.execute_math_query(query)\n",
        "            elif any(word in query.lower() for word in ['top', 'maximum', 'minimum', 'group by']):\n",
        "                return self.data_query_module.execute_query(query)\n",
        "            else:\n",
        "                return self.qa_module.get_answer(query, self.eda_report)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {str(e)}\")\n",
        "            return str(e)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv('/content/Salary_Data.csv')\n",
        "    eda = EDAModule(df)\n",
        "\n",
        "    # Generate Full Report\n",
        "    report = eda.generate_full_report()\n",
        "\n",
        "    # Save Insights\n",
        "    eda.save_insights()\n",
        "\n",
        "    chatbot = DataDrivenChatbot(df)\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"Ask a question: \")\n",
        "        if user_query.lower() == \"exit\":\n",
        "            break\n",
        "        response = chatbot.process_query(user_query)\n",
        "        print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JvwFRNNKpM-",
        "outputId": "62d1c6d6-37f1-45dc-b5b0-09d06bf88483"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question: Lowest Salary\n",
            "37731\n",
            "Ask a question: Average Age\n",
            "27.216666666666665\n",
            "Ask a question: Highest age\n",
            "38.0\n",
            "Ask a question: Show me the top 5 rows where column Age > 25\n",
            "    YearsExperience   Age  Salary\n",
            "16              5.1  26.0   66029\n",
            "17              5.3  27.0   83088\n",
            "18              5.9  28.0   81363\n",
            "19              6.0  29.0   93940\n",
            "20              6.8  30.0   91738\n",
            "Ask a question: What is the maximum value in column Salary?\n",
            "122391\n",
            "Ask a question: What is the standard deviation of column Age?\n",
            "5.161267108077196\n",
            "Ask a question: exit\n"
          ]
        }
      ]
    }
  ]
}